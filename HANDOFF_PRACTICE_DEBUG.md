# ðŸ› HANDOFF: Practice Page Debug - Gemini Live Not Working

**Date:** November 18, 2025  
**Issue:** Gemini Live assistant not functioning on Practice page  
**Priority:** CRITICAL  
**File:** `client/src/pages/PracticeLive.tsx`

---

## ðŸ“‹ Issue Summary

The newly implemented unified Gemini Live assistant on the Practice page is not working as expected:

**Expected Behavior:**
1. âœ… User selects difficulty level (Beginner/Intermediate/Advanced)
2. âœ… User clicks "Start Session" button
3. âŒ **Gemini should introduce the word** (e.g., "Great! Let's practice the word 'Book'. This is a beginner-level word...")
4. âŒ User clicks Record button â†’ records for 5 seconds
5. âŒ **Gemini should analyze and provide feedback** (e.g., "Well done! Your pronunciation was clear...")

**Actual Behavior:**
1. âœ… User selects difficulty level â†’ word appears
2. âœ… User clicks "Start Session" â†’ status changes to "Connected"
3. âŒ **No audio from Gemini** (no word introduction)
4. âœ… User clicks Record â†’ countdown works, waveform animates
5. âŒ **No feedback after recording** (silence)

---

## ðŸŽ¯ Expected Flow

### 1. Session Initialization
```typescript
// When user clicks "Start Session"
startSession() â†’ {
  1. Create GoogleGenAI client
  2. Connect to Gemini Live with system prompt
  3. Session opens (onopen callback)
  4. Start microphone streaming
  5. ðŸ”Š Gemini should automatically speak the introduction
}
```

**System Prompt Includes:**
- Current word: "Book"
- Difficulty: "beginner"
- Meaning: "A written work"
- Example: "I read a book every week"
- Smart Context (RAG): IELTS criteria, pronunciation guides
- **Instruction:** "Start by introducing the word 'Book' and explaining how to pronounce it!"

### 2. Expected Introduction (Gemini should say this automatically)
> "Great! Let's practice the word 'Book'. This is a beginner-level word with a short 'oo' sound. Make sure to round your lips for the 'oo' - it's different from the 'uh' sound in 'buck'. The 'B' sound is a voiced bilabial plosive - press your lips together and release. Ready to try?"

### 3. User Records
- User clicks Record button
- Audio streams to Gemini via `sendRealtimeInput()`
- Countdown: 5 â†’ 4 â†’ 3 â†’ 2 â†’ 1 â†’ Auto-stop
- Waveform animation shows audio level

### 4. Expected Feedback (Gemini should say this after recording)
> "Well done! Your 'B' sound was clear and strong. The vowel 'oo' was good, but try rounding your lips a bit more. Let's try it again, or move to the next word!"

---

## ðŸ” Technical Context

### File Structure
```
client/src/pages/PracticeLive.tsx  â† Main file (470 lines)
â”œâ”€â”€ State Management (lines 18-32)
â”‚   â”œâ”€â”€ Practice state (word, difficulty, score)
â”‚   â””â”€â”€ Gemini Live state (status, transcripts, audioLevel, isSpeaking, isRecording)
â”œâ”€â”€ Refs (lines 34-47)
â”‚   â”œâ”€â”€ sessionPromiseRef
â”‚   â”œâ”€â”€ inputAudioContextRef / outputAudioContextRef
â”‚   â”œâ”€â”€ mediaStreamRef
â”‚   â””â”€â”€ Audio nodes (processor, analyser, source)
â”œâ”€â”€ startSession() (lines 139-268)
â”‚   â”œâ”€â”€ Create system prompt with word context
â”‚   â”œâ”€â”€ Connect to Gemini Live
â”‚   â”œâ”€â”€ onopen: Start microphone streaming
â”‚   â”œâ”€â”€ onmessage: Handle transcriptions and audio
â”‚   â””â”€â”€ onerror: Handle errors
â””â”€â”€ UI Components (lines 327-505)
    â”œâ”€â”€ Difficulty selection
    â”œâ”€â”€ Current word display
    â”œâ”€â”€ TalkyTalky Coach card
    â”œâ”€â”€ Waveform animation
    â”œâ”€â”€ Conversation history
    â””â”€â”€ Control buttons
```

### Gemini Live API Configuration
```typescript
sessionPromiseRef.current = ai.live.connect({
  model: 'gemini-2.5-flash-native-audio-preview-09-2025',
  config: {
    systemInstruction: { parts: [{ text: systemPrompt }] },
  },
  callbacks: {
    onopen: async () => { /* Microphone setup */ },
    onmessage: async (message: LiveServerMessage) => { /* Handle responses */ },
    onerror: (error: any) => { /* Error handling */ },
  },
});
```

### Audio Handling

**Input (User â†’ Gemini):**
```typescript
// Line 197-209: Audio processor sends PCM data
audioProcessorNodeRef.current.onaudioprocess = (audioProcessingEvent) => {
  const inputData = audioProcessingEvent.inputBuffer.getChannelData(0);
  const pcmBlob: GenAIBlob = {
    data: encode(new Uint8Array(new Int16Array(inputData.map(f => f * 32768)).buffer)),
    mimeType: 'audio/pcm;rate=16000',
  };
  
  if (sessionPromiseRef.current && isRecording) {
    sessionPromiseRef.current.then((session) => {
      session.sendRealtimeInput({ media: pcmBlob });
    });
  }
};
```

**Output (Gemini â†’ User):**
```typescript
// Line 235-256: Audio playback from Gemini
const base64Audio = message.serverContent?.modelTurn?.parts?.[0]?.inlineData?.data;
if (base64Audio && outputAudioContextRef.current) {
  setIsSpeaking(true);
  const audioBuffer = await decodeAudioData(decode(base64Audio), outputAudioContextRef.current, 24000, 1);
  const source = outputAudioContextRef.current.createBufferSource();
  source.buffer = audioBuffer;
  source.connect(outputAudioContextRef.current.destination);
  source.start(startTime);
}
```

---

## ðŸ› Potential Issues

### Issue 1: Gemini Not Speaking After Session Opens
**Hypothesis:** System prompt might not be triggering automatic response

**Possible Causes:**
1. âŒ System instruction format incorrect
2. âŒ Gemini Live requires explicit "send message" after connection
3. âŒ Model not configured for automatic speech output
4. âŒ Missing speech configuration in `config`

**Debug Steps:**
1. Check if `onopen` callback is firing (add console.log)
2. Check if we need to send an initial message after connection
3. Compare with working AICoach.tsx implementation (lines 140-250)
4. Verify audio output context is properly initialized

**AICoach.tsx Working Example (for reference):**
```typescript
// Line 231-243 in AICoach.tsx
config: {
  speechConfig: {
    voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Zephyr' } },
  },
  systemInstruction: { 
    parts: [{ 
      text: smartContext 
        ? `${TALKYTALKY_SYSTEM_PROMPT}\n\n${smartContext}`
        : TALKYTALKY_SYSTEM_PROMPT 
    }] 
  },
  inputAudioTranscription: {},
  outputAudioTranscription: {},
},
```

### Issue 2: No Feedback After Recording
**Hypothesis:** Gemini not receiving or processing user audio

**Possible Causes:**
1. âŒ `sendRealtimeInput()` not working during `isRecording`
2. âŒ Audio format mismatch (PCM encoding issue)
3. âŒ `turnComplete` not triggering properly
4. âŒ Transcription not being captured

**Debug Steps:**
1. Add console.log in `onaudioprocess` to verify audio is being sent
2. Check if `message.serverContent?.inputTranscription` is populated
3. Check if `message.serverContent?.turnComplete` is firing
4. Verify `isRecording` state is true during recording

### Issue 3: Missing Speech Configuration
**Hypothesis:** PracticeLive.tsx missing speech config that AICoach.tsx has

**Key Difference:**
```typescript
// AICoach.tsx HAS this:
speechConfig: {
  voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Zephyr' } },
},

// PracticeLive.tsx MISSING this âŒ
```

**Fix:** Add speech config to PracticeLive.tsx

---

## ðŸ”§ Debugging Checklist

### Step 1: Verify Session Connection
```typescript
// Add to onopen callback (line 183)
console.log('âœ… Gemini Live session opened');
console.log('ðŸ“ System prompt:', systemPrompt);
console.log('ðŸŽ¤ Microphone stream:', mediaStreamRef.current);
console.log('ðŸ”Š Output audio context:', outputAudioContextRef.current);
```

### Step 2: Check Message Handling
```typescript
// Add to onmessage callback (line 217)
console.log('ðŸ“¨ Received message:', message);
console.log('ðŸ—£ï¸ Output transcription:', message.serverContent?.outputTranscription);
console.log('ðŸŽ§ Input transcription:', message.serverContent?.inputTranscription);
console.log('âœ… Turn complete:', message.serverContent?.turnComplete);
console.log('ðŸŽµ Audio data:', message.serverContent?.modelTurn?.parts?.[0]?.inlineData?.data ? 'YES' : 'NO');
```

### Step 3: Verify Audio Streaming
```typescript
// Add to onaudioprocess (line 197)
console.log('ðŸŽ¤ Sending audio, isRecording:', isRecording);
```

### Step 4: Check for Errors
```typescript
// Add to onerror callback (line 264)
console.error('âŒ Gemini Live error:', error);
console.error('ðŸ“Š Error details:', JSON.stringify(error, null, 2));
```

---

## ðŸŽ¯ Comparison: AICoach vs PracticeLive

| Feature | AICoach.tsx (âœ… Working) | PracticeLive.tsx (âŒ Not Working) |
|---------|-------------------------|----------------------------------|
| **Speech Config** | âœ… Has `speechConfig` with voice | âŒ Missing `speechConfig` |
| **Transcription** | âœ… Has `inputAudioTranscription` | âŒ Missing in config |
| **Transcription** | âœ… Has `outputAudioTranscription` | âŒ Missing in config |
| **System Prompt** | âœ… Uses constant + smart context | âœ… Uses inline prompt + smart context |
| **Audio Playback** | âœ… Same implementation | âœ… Same implementation |
| **Recording** | âœ… Manual start/stop | âœ… Auto-stop with countdown |

**Key Missing Pieces in PracticeLive.tsx:**
1. âŒ `speechConfig` with voice configuration
2. âŒ `inputAudioTranscription: {}`
3. âŒ `outputAudioTranscription: {}`

---

## ðŸ› ï¸ Suggested Fix

### Fix 1: Add Missing Configuration
```typescript
// Line 176-180: Update config
sessionPromiseRef.current = ai.live.connect({
  model: 'gemini-2.5-flash-native-audio-preview-09-2025',
  config: {
    speechConfig: {
      voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Zephyr' } },
    },
    systemInstruction: { parts: [{ text: systemPrompt }] },
    inputAudioTranscription: {},
    outputAudioTranscription: {},
  },
  callbacks: { /* ... */ },
});
```

### Fix 2: Send Initial Message (if needed)
```typescript
// After onopen, send initial message to trigger response
onopen: async () => {
  console.log('Gemini Live session opened');
  setStatus(AppStatus.CONNECTED);
  
  // ... microphone setup ...
  
  // Send initial message to trigger introduction
  const session = await sessionPromiseRef.current;
  session.send({ text: "Please introduce the current word." });
},
```

### Fix 3: Verify Audio Output
```typescript
// Ensure outputAudioContextRef is properly initialized
outputAudioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ 
  sampleRate: 24000,
  latencyHint: 'interactive', // Add this for better responsiveness
});
```

---

## ðŸ“Š Test Plan

### Test 1: Session Connection
1. Open Practice page
2. Select Beginner level
3. Click "Start Session"
4. **Expected:** Console shows "âœ… Gemini Live session opened"
5. **Expected:** Status changes to "Connected"
6. **Expected:** Gemini speaks introduction within 2-3 seconds

### Test 2: Word Introduction
1. After session connects
2. **Expected:** Hear Gemini say: "Great! Let's practice the word 'Book'..."
3. **Expected:** See transcript appear in conversation history
4. **Expected:** `isSpeaking` state becomes true during speech

### Test 3: Recording and Feedback
1. Click Record button
2. Say the word "Book"
3. Wait for 5-second countdown to complete
4. **Expected:** Recording stops automatically
5. **Expected:** Gemini responds within 2-3 seconds
6. **Expected:** Hear feedback like "Well done! Your pronunciation was clear..."
7. **Expected:** See user transcript (blue) and Gemini response (purple)

### Test 4: Conversation Flow
1. Complete one word
2. Click "Next Word"
3. **Expected:** Session restarts with new word
4. **Expected:** Gemini introduces new word
5. **Expected:** Full conversation history preserved

---

## ðŸ”— Related Files

### Working Reference (AI Coach)
- `client/src/pages/AICoach.tsx` (lines 140-250) - Working Gemini Live implementation
- Uses same audio handling, but has `speechConfig`

### Audio Utilities
- `client/src/utils/audio.ts` - encode/decode/decodeAudioData functions
- Used for PCM audio conversion

### Components
- `client/src/components/VoiceWaveform.tsx` - Waveform animation (working)
- `client/src/components/TalkyLogo.tsx` - Logo component (working)

### Backend (RAG)
- `server/ragRouter.ts` (line 43-68) - `getSmartContext` endpoint
- `server/ragService.ts` (line 67-130) - Smart Context retrieval logic

---

## ðŸ’¡ Additional Notes

### Why This Architecture?
The unified Gemini Live assistant was designed to replace separate API calls:
- **Old:** `analyzePronunciation` API + `generateSpeech` API (2 separate requests)
- **New:** One Gemini Live session (continuous conversation)

### Benefits (When Working)
1. Natural conversational flow
2. Context-aware coaching
3. Real-time feedback
4. No API quota issues (one session)
5. Personalized tips using RAG

### Current Status
- âœ… UI working (word display, buttons, waveform)
- âœ… Recording working (countdown, audio capture)
- âœ… RAG integration working (Smart Context fetched)
- âŒ Gemini not speaking (no introduction, no feedback)
- âŒ Audio output not working

---

## ðŸŽ¯ Success Criteria

**Fix is successful when:**
1. âœ… User clicks "Start Session" â†’ Gemini introduces word within 3 seconds
2. âœ… User records pronunciation â†’ Gemini provides feedback within 3 seconds
3. âœ… Conversation history shows both user and Gemini transcripts
4. âœ… Audio playback works (hear Gemini's voice)
5. âœ… Multiple words work (session persists or restarts cleanly)

---

## ðŸ“ž Contact Context

**User Feedback:**
> "She is not working on practice pages. No announcement no feedback after recording."

**Translation:**
- "She" = Gemini Live assistant
- "No announcement" = No word introduction when session starts
- "No feedback after recording" = No response after user records pronunciation

**User Expectation:**
Same conversational experience as AI Coach page, but focused on pronunciation practice with specific words.

---

## ðŸš€ Next Steps for Gemini

1. **Add missing configuration** (`speechConfig`, transcription configs)
2. **Add debug logging** to verify session connection and message flow
3. **Test audio output** to ensure Gemini's voice is heard
4. **Verify recording** to ensure user audio reaches Gemini
5. **Test full flow** from introduction â†’ recording â†’ feedback
6. **Compare with AICoach.tsx** to identify any other missing pieces

Good luck! ðŸŽ¯
